{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drikus-d/unsupervised-predict-streamlit-template/blob/main/testing_Predict_Notebook_Team_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j0k5KTHtAwJ"
      },
      "source": [
        "# Group 5: Movie Recommender Challenge: Unsupervised Learning - EDSA\n",
        "\n",
        "@ Team 5\n",
        "\n",
        "<div align=\"center\" style=\"width: 800px; font-size: 100%; text-align: center; margin: 0 auto\">\n",
        "<img src=\"https://raw.githubusercontent.com/drikus-d/unsupervised-predict-streamlit-template/master/DataSets/reccomnd%20pic.jpg\"\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmpXZtlmtAwM"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. <a href =\"#1.-Challenge\">Challenge Description</a>\n",
        "2. <a href =\"#2.-Evaluation-Metric\">Evaluation Metric</a>\n",
        "3. <a href =\"#3.-Comet-Experiment\">Comet Experiment</a>\n",
        "4. <a href =\"#4.-Importing-Libraries\">Importing Libraries</a>\n",
        "5. <a href =\"#5.- Reading the Datasets\"> Reading the Datasets</a>\n",
        "6. <a href =\"#6.-Data Description & Summary Statistics\">Data Description & Summary Statistics</a>\n",
        "7. <a href =\"#7.-Exploratory-Data-Analysis\"> Exploratory Data Analysis</a>\n",
        "8. <a href =\"#7.-Exploratory-Data-Analysis\"> Data Engineering Post EDA </a>\n",
        "9. <a href =\"#8.-Recommendation-System:-Content-Based-Filtering\">Recommendation System</a>\n",
        "10. <a href =\"#9.-Dimensionality-Reduction\">Dimensionality Reduction</a>\n",
        "11. <a href =\"#10.-Collaborative-Filtering\">Collaborative Filtering</a>\n",
        "12.  <a href =\"#10.-The Chosen Model Summary\">The Chosen Model Summary</a>\n",
        "13. <a href =\"#11.-Conclusion\">Conclusion</a>\n",
        "14. <a href =\"#12.-Submission\">Submission</a>\n",
        "15. <a href =\"#12.-Recommendations\">Future Recommendations</a>  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YIUclHVtAwN"
      },
      "source": [
        "## 1. Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQloHv60tAwO"
      },
      "source": [
        "##**<font color='brown'>Description:</font>**\n",
        "<p align=\"justify\" > \n",
        "In todayâ€™s technology driven world, recommender systems are socially and economically critical for ensuring that individuals can make appropriate choices surrounding the content they engage with on a daily basis. One application where this is especially true surrounds movie content recommendations; where intelligent algorithms can help viewers find great titles from tens of thousands of options.\n",
        "\n",
        "<p align=\"justify\" > With this context, EDSA is challenging US to construct a recommendation algorithm based on content or collaborative filtering, capable of accurately predicting how a user will rate a movie they have not yet viewed based on their historical preferences.\n",
        "\n",
        "<p align=\"justify\" > Providing an accurate and robust solution to this challenge has immense economic potential, with users of the system being exposed to content they would like to view or purchase - generating revenue and platform affinity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**<font color='cyan'>Problem Statement:</font>**\n",
        "<p align=\"justify\" > Develop an unsupervised machine learning model that can accurately predict how a user would rate a movie they haven't seen based on their previous browsing history and/or content or collaborative filtering.\n",
        "\n",
        "##**<font color='purple'>Task:</font>**\n",
        "<p align=\"justify\" > To construct a recommendation algorithm based on content or collaborative filtering, capable of accurately predicting how a user will rate a movie they have not yet viewed based on their historical preferences.\n"
      ],
      "metadata": {
        "id": "VbwBesQxUgFY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8QI8KKetAwO"
      },
      "source": [
        "## 2. Evaluation Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd8zGSP5tAwP"
      },
      "source": [
        "<p align=\"justify\" > The evaluation metric for this competition is Root Mean Square Error. Root Mean Square Error (RMSE) is commonly used in regression analysis and forecasting, and measures the standard deviation of the residuals arising between predicted and actual observed values for a modelling process. For our task of generating user movie ratings via recommendation algorithms, the the formula is given by:\n",
        "\n",
        "![RMSE.PNG](https://github.com/drikus-d/unsupervised-predict-streamlit-template/blob/main/resources/imgs/RMSE.png?raw=1)\n",
        "\n",
        "<p align=\"justify\" > Where R is the total number of recommendations generated for users and movies, with ${r_{ui}}$ and ${r-hat_{ui}}$ being the true, and predicted ratings for user u watching movie i, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzsONOt2tAwP"
      },
      "source": [
        "## 3. Comet Experiment "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI8jeLZftAwQ"
      },
      "outputs": [],
      "source": [
        "# import comet_ml at the top of your file\n",
        "from comet_ml import Experiment\n",
        "\n",
        "# Create an experiment with your api key\n",
        "experiment = Experiment(\n",
        "    api_key=\"1DpubPFI6szNQR3Dou0CiLiDC\",\n",
        "    project_name=\"recommender\",\n",
        "    workspace=\"drikus-d\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x06K9s1xtAwS"
      },
      "source": [
        "## 4. Importing Libraries\n",
        "\n",
        "Import supporting python libraries to help construct our recommendation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtdS_otatAwS"
      },
      "outputs": [],
      "source": [
        "# Install packages here\n",
        "# Packages for data processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from sklearn import preprocessing\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "from scipy.sparse import csr_matrix\n",
        "import scipy as sp\n",
        "\n",
        "\n",
        "# Packages for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "#Modelling \n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "\n",
        "# Packages for modeling\n",
        "from surprise import Reader\n",
        "from surprise import Dataset\n",
        "from surprise import KNNWithMeans\n",
        "from surprise import KNNBasic\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise.model_selection import GridSearchCV\n",
        "from surprise import SVD\n",
        "from surprise import SVDpp\n",
        "from surprise import NMF\n",
        "from surprise import SlopeOne\n",
        "from surprise import CoClustering\n",
        "from sklearn.neighbors import  NearestNeighbors\n",
        "import heapq\n",
        "\n",
        "# Packages for model evaluation\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from time import time\n",
        "\n",
        "# Package to suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Packages for saving models\n",
        "import pickle\n",
        "\n",
        "import cufflinks as cf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Packages for Randomisation\n",
        "import random\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted',\n",
        "        rc={'figure.figsize': (15,10)})\n",
        "\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "init_notebook_mode(connected=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9CYsj3rtAwT"
      },
      "source": [
        "## 5. Reading the Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhYhExzEtAwU"
      },
      "outputs": [],
      "source": [
        "movies_df = pd.read_csv('./movies.csv') \n",
        "imdb_df = pd.read_csv('./imdb_data.csv')\n",
        "genome_score = pd.read_csv('./genome_scores.csv')\n",
        "genome_tags = pd.read_csv('./genome_tags.csv')\n",
        "train_df = pd.read_csv('./train.csv')\n",
        "test_df = pd.read_csv('./test.csv')\n",
        "tags_df = pd.read_csv('./tags.csv')\n",
        "links_df = pd.read_csv('./links.csv')\n",
        "sample_submission_df = pd.read_csv('./sample_submission.csv') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGIKjQhytAwU"
      },
      "source": [
        "#### Kaggle \n",
        "<p align=\"justify\" > The aim of the Kaggle competition is to determine the rating score for the respective movieId. \n",
        "\n",
        "#### The Kaggle submision file format \n",
        "<p align=\"justify\" > The submission files should contain two columns: Id and rating. Where:\n",
        "'Id' is a concatenation of the userID and movieID given in the test file while 'rating' is the predicted rating for a given user-movie pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lD6OkOXtAwV"
      },
      "outputs": [],
      "source": [
        "sample_submission_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJi3AmcntAwV"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Displaying the datasets\n",
        "Displaying the different datasets along with their summary statistics and investigating the null values for each data."
      ],
      "metadata": {
        "id": "lISnoMHqYuqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of unique entries for features of interest\n",
        "unique_movies = len(movies['movieId'].unique().tolist())\n",
        "unique_tags = len(tags['tag'].unique().tolist())\n",
        "unique_users = len(train['userId'].unique().tolist())\n",
        "unique_directors = len(imdb_data['director'].unique().tolist())\n",
        "unique_actors = len(imdb_data['title_cast'].unique().tolist())\n",
        "\n",
        "# Create a dataframe to store the number of unique entries for features of interest\n",
        "unique = pd.DataFrame({\"movies\": [unique_movies],\n",
        "                       \"tags\" : [unique_tags],\n",
        "                       \"users\": [unique_users],\n",
        "                       \"directors\" : [unique_directors],\n",
        "                       \"actors\" : [unique_actors]}, index=['unique_entries'])\n",
        "\n",
        "print('number of unique entries for features of interest')\n",
        "display(unique.transpose())\n",
        "\n",
        "# Determine and display the min and max ratings received\n",
        "min_rating = train.rating.min()\n",
        "max_rating = train.rating.max()\n",
        "print('Lowest rating: {}'.format(min_rating))\n",
        "print('Highest rating: {}'.format(max_rating))\n"
      ],
      "metadata": {
        "id": "J10XR5ZwYVNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOGLKA2ctAwV"
      },
      "source": [
        "### Train and Test data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-WQwytHtAwW"
      },
      "outputs": [],
      "source": [
        "# View train data info, shape and content  \n",
        "display(train_df.head(3))\n",
        "display(train_df.info()) # Get the summary of the dataset's metadata\n",
        "print(train_df.isnull().sum()) # check if there are any null values)\n",
        "print('..........................')\n",
        "\n",
        "# View test data info, shape and content  \n",
        "display(test_df.head(3))\n",
        "display(test_df.info()) # Get the summary of the dataset's metadata\n",
        "print(test_df.isnull().sum()) # check if there are any null values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA78fYC3tAwW"
      },
      "source": [
        "The train and test datasets contains numerical data types i.e. 10 000 038 observations in train and 5 000 019 in test datasets.<br>\n",
        "Both datasets contains zero null values<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTkH8Sn0tAwW"
      },
      "source": [
        "### Movies data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbbb2tZZtAwW"
      },
      "outputs": [],
      "source": [
        "# View movies data info and shape and content head \n",
        "display(movies_df.head())\n",
        "movies_df.info() # Get the summary of the dataset's metadata\n",
        "print(movies_df.isnull().sum()) # check if there are any null values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4PFJ6JYtAwW"
      },
      "source": [
        "Movies dataset: numerical data types, 62423 observations, no null values in the dataset <br>\n",
        "Movie dataset contains extra movie info such as the title and genre "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tfrbMtAwX"
      },
      "source": [
        "### Genome Scores and tags "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHzOrx87tAwX"
      },
      "source": [
        "The tag genome encodes how strongly movies exhibit particular properties represented by tags (atmospheric, thought-provoking, realistic, etc.). The tag genome was computed using a machine learning algorithm on user-contributed content including tags, ratings, and textual reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbKQL_cytAwX"
      },
      "outputs": [],
      "source": [
        "display(genome_tags.head()) # Tags - what do they discribe? \n",
        "genome_tags.info() # Summary of the Genome_tag Dataframe \n",
        "genome_score.info() # Summary of the Genome_score Dataframe \n",
        "\n",
        "print(genome_tags.isnull().sum())\n",
        "print(genome_score.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMBx4tMitAwX"
      },
      "source": [
        "<p align=\"justify\" > More than 15 million genome scores make up the genome_score dataset i.e. 1128 observations in the genome tags data. <br>These datasets gives us the tags and their relative score. <br>The genome tag df consists of the tagId as well the tag which is a string<br>\n",
        "No null values in either the tags or score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulhyAy_CtAwY"
      },
      "source": [
        "### Imdb database of movies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp06reZjtAwY"
      },
      "source": [
        "Additional movie content data obtained from [IMDB](https://www.imdb.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEhL1rNptAwY"
      },
      "outputs": [],
      "source": [
        "display(imdb_df.head()) \n",
        "display(imdb_df.info()) # Summary of imdb database\n",
        "print(imdb_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32qfjN1QtAwY"
      },
      "source": [
        "<p align=\"justify\" > There are 62423 movies in the movies database and only 27278 in the imdb dataset.<br>\n",
        "The IMBD dataset gives more infomation to about 27000+ movies present in the train dataset. The features are title_cast, director, runtime, budget as well plot_keywords. The title_cast feature describes the most famous/paid actors/actresses in the movie and runtime describes how long movie was and plot_keyword describes the most important key words of the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntvxc10otAwZ"
      },
      "source": [
        "## 7. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1riRUUSbtAwZ"
      },
      "source": [
        "We investigate through EDA four areas to get insights before we tackle modelling:  \n",
        "\n",
        "* **Rating Analysis**\n",
        "* **User Analysis**\n",
        "* **Year Analysis**\n",
        "* **Genre Analysis**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amLmhGl0tAwZ"
      },
      "source": [
        "### Ratings \n",
        "The train dataset gives an overview of how each user rated each movie. What was observed that each user can rate a movie from 0.5 (Worst) to 5.0 (Best). Here is the full detailed scale:\n",
        " * 0.5 - Very Bad\n",
        " * 1.0 - Bad\n",
        " * 1.5 - Moderately Bad\n",
        " * 2.0 - Not that Bad\n",
        " * 2.5 - Less than average \n",
        " * 3.0 - Average \n",
        " * 3.5 - Above than average \n",
        " * 4.0 - Good\n",
        " * 4.5 - Very Good\n",
        " * 5.0 - Excellent \n",
        " <br>\n",
        " \n",
        "**How many movies whererated there per rating category?**\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUMcpjvGtAwZ"
      },
      "outputs": [],
      "source": [
        "# Plotting the graph\n",
        "fig, ax = plt.subplots(figsize=(20, 10)) # Initialize the plot with set figure size\n",
        "\n",
        "# Create a countplot to visualise the number of movies per category\n",
        "sns.countplot(ax=ax, x='rating', data=train_df, color=(0.2, 0.4, 0.6, 0.6)) \n",
        "plt.xlabel('Rating', fontsize=25)\n",
        "plt.ylabel('Count', fontsize=25)\n",
        "plt.xticks(size = 15)\n",
        "plt.yticks(size = 15)\n",
        "fig.suptitle(\"Movies count per rating\", fontsize=30)\n",
        "plt.ticklabel_format(style='plain', axis='y', useOffset=False) # Set the tick labels to appear in non-scientific form\n",
        "\n",
        "# Make the counts appear on the different bars\n",
        "for p in ax.patches:\n",
        "    ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()), size = 16)\n",
        "    \n",
        "# Show the countplot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxtndF_EtAwZ"
      },
      "source": [
        "Most movies where scored with a rating of 4 <br><br>\n",
        "**Is there a correlation between how many times movies where rated and the rating?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMa7TsZetAwa"
      },
      "outputs": [],
      "source": [
        "train_movies = train_df.merge(movies_df,on = 'movieId') # Merges the movie and train datasets\n",
        "train_movies.drop(columns=['timestamp'],inplace=True) # Dropping the timestamp column\n",
        "train_movies.head(10) # Shows the first 10 observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7BaqELftAwa"
      },
      "outputs": [],
      "source": [
        "# Create a new dataframe from the train_movies dataframe to get the average rating  \n",
        "train2 = train_movies.copy()\n",
        "\n",
        "# Drop any duplicated information \n",
        "train2.drop_duplicates(['title','genres'],inplace = True)\n",
        "\n",
        "# Get the  avarage rating for each movie in the data \n",
        "total_ratings = train_movies.groupby(['movieId','genres']).mean()['rating'].reset_index()\n",
        "\n",
        "# Merge the the train2 and the total_ratings to create a new data \n",
        "train2 = train2.merge(total_ratings, on = 'movieId')\n",
        "\n",
        "train2.drop(columns=['userId','rating_x','genres_y'],inplace=True) # Delete the duplicated columns\n",
        "\n",
        "train2.rename(columns={'genres_x':'genres','rating_y':'rating'},inplace=True) # Rename the columns\n",
        "\n",
        "train3 = train2.copy() # Make a copy of the Train2 dataset\n",
        "# Create a user dataset to check how many time it has been voted for\n",
        "user_rated = pd.DataFrame(train_df['movieId'].value_counts().reset_index()) # Create a user dataframe using groupby function\n",
        "\n",
        "user_rated.rename(columns = {'index':'movieId','movieId':'voted'},inplace = True) # Rename the columns \n",
        "\n",
        "train3 = train3.merge(user_rated, on ='movieId') # Combine the train3 dataset with the User_rated data\n",
        "\n",
        "# Create a dataframe to visualize the most rated movies and least rated movies\n",
        "train4 = train3.sort_values('voted',ascending=False)\n",
        "\n",
        "# Check for correlation between rating and number of the times have the user rated a movie\n",
        "\n",
        "#Instantiate the figures\n",
        "fig,ax = plt.subplots(figsize =(20,10))\n",
        "\n",
        "# Create a scatter plot to visualise \n",
        "sns.regplot(data = train3, x = \"rating\", y = \"voted\",line_kws={\"color\": \"red\"})\n",
        "ax.set_title(\"A scatter plot showing the relationship between the average rating for each vs number of votes per movie\")\n",
        "# Show the scatterplot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztNd7upOtAwa"
      },
      "source": [
        "The more the users rated for a movie, the average rating of the movie seems to increase, similarly for movies that have the least movies which have a lower rating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OezbEkLytAwb"
      },
      "source": [
        "### Users "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gam8A4MtAwb"
      },
      "outputs": [],
      "source": [
        "# Create a Dataframe consisting of the users average rating the give per user and number of times they have rated movies\n",
        "train1 = pd.DataFrame(train_df.groupby('userId')['rating'].agg('mean').reset_index())\n",
        "train1_2 = pd.DataFrame(train_df.groupby('userId')['rating'].count().reset_index())\n",
        "# Merge the above to dataframe \n",
        "train1 = train1.merge(train1_2,on='userId',how = 'left')\n",
        "# Rename the columns respectively\n",
        "train1.rename(columns={'rating_x':'avg_rating','rating_y':'number_of_movies'},inplace = True)\n",
        "\n",
        "# Sort the data in descending of the number of movies the user has rated\n",
        "train1 = train1.sort_values('number_of_movies', ascending = False)\n",
        "\n",
        "# Check for correlation between average rating and the number of the times have the user has watch a movie\n",
        "\n",
        "# Create a scatter plot to visualise \n",
        "sns.regplot(data = train1, y = \"number_of_movies\", x = \"avg_rating\",line_kws={\"color\": \"red\"})\n",
        "\n",
        "# Show the scatterplot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AM8XPfAtAwb"
      },
      "source": [
        "### Genres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-0aJZZltAwb"
      },
      "outputs": [],
      "source": [
        "# Create dataframe containing only the movieId and genres\n",
        "movies_genres = pd.DataFrame(movies_df[['movieId', 'genres']],\n",
        "                             columns=['movieId', 'genres'])\n",
        "\n",
        "# Split genres seperated by \"|\" and create a list containing the genres allocated to each movie\n",
        "movies_genres.genres = movies_genres.genres.apply(lambda x: x.split('|'))\n",
        "\n",
        "# Create expanded dataframe where each movie-genre combination is in a seperate row\n",
        "movies_genres = pd.DataFrame([(tup.movieId, d) for tup in movies_genres.itertuples() for d in tup.genres],\n",
        "                             columns=['movieId', 'genres'])\n",
        "\n",
        "print(movies_genres['genres'].unique())\n",
        "\n",
        "movies_genres.head()\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "gen = movies_genres['genres'].explode()\n",
        "text = list(set(gen))\n",
        "plt.rcParams['figure.figsize'] = (13, 13)\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=100,background_color=\"black\").generate(str(text))\n",
        "\n",
        "plt.imshow(wordcloud,interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3dam0y8tAwb"
      },
      "outputs": [],
      "source": [
        "# Ploting top genres in the Dataset\n",
        "plt.figure(figsize=(20, 10))\n",
        "gen = movies_genres['genres'].explode()\n",
        "ax=sns.countplot(x=gen, order=gen.value_counts().index[:30],color=(0.2, 0.4, 0.6, 0.6))\n",
        "plt.xlabel('Genres', fontsize=25)\n",
        "plt.ylabel('Count', fontsize=25)\n",
        "plt.xticks(size = 15)\n",
        "plt.yticks(size = 15)\n",
        "ax.set_title('Top Genres', fontsize=30)\n",
        "plt.xticks(rotation =90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4keQERetAwc"
      },
      "source": [
        "The top genres are Drama and Comedy. Movies however can have multiple genres "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xS6hQvvtAwc"
      },
      "source": [
        "## 8. Data Engineering Post EDA "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeHQ1RNAtAwc"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('./train.csv')\n",
        "# remove outlier in userID in train_df\n",
        "display(train_df.info())\n",
        "display(train_df.head(3))\n",
        "\n",
        "train_df.drop(train_df.index[train_df['userId'] == 72315], inplace=True)\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "\n",
        "display(train_df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Recommender Systems"
      ],
      "metadata": {
        "id": "Oc_tQgs0_EoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\" > Recommendation engines are a subclass of machine learning which generally deal with ranking or rating products / users. A recommender system is a system which predicts ratings a user might give to a specific item. These predictions will then be ranked and returned back to the user."
      ],
      "metadata": {
        "id": "COoDJ0Oe_NOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of recommendation systems:\n",
        "* Popularity Based\n",
        "\n",
        "It keeps a track of view counts for each movie/video and then lists movies based on views in descending order.\n",
        "\n",
        "* Content-based filtering \n",
        "\n",
        "Based on the user's previous activities or explicit feedback, content-based filtering recommends other products that are similar to what users like.\n",
        "\n",
        "* Collaborative filtering\n",
        "\n",
        "In other words, the recommendations get filtered based on the collaboration between similar userâ€™s preferences.\n",
        "\n",
        "**We will implement a Content-based filtering and Collaborative filtering system in this project:**"
      ],
      "metadata": {
        "id": "RbJbPo4j_Z8P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzFkGDwItAwc"
      },
      "source": [
        "## 9. Content Based Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\" > Content-based filtering uses item features to recommend other items similar to what the user likes, based on their previous actions or explicit feedback."
      ],
      "metadata": {
        "id": "NaEmwmL_-1Ni"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXUyEy8ctAwc"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing(subset_size):\n",
        "    \"\"\"Prepare data for use within Content filtering algorithm.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    subset_size : int\n",
        "        Number of movies to use within the algorithm.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Pandas Dataframe\n",
        "        Subset of movies selected for content-based filtering.\n",
        "\n",
        "    \"\"\"\n",
        "    movies = movies_df.copy()\n",
        "    \n",
        "    # Split the genres feature so that each genres will be in a list format \n",
        "    movies['genres'] = movies.genres.str.split('|')\n",
        "\n",
        "    #Copying the movie dataframe into a new one since we won't need to use the genre information in our first case.\n",
        "    moviesWithGenres_df = movies.copy()\n",
        "\n",
        "    #For every row in the dataframe, iterate through the list of genres and place a 1 into the corresponding column\n",
        "    for index, row in movies.iterrows():\n",
        "        for genre in row['genres']:\n",
        "            moviesWithGenres_df.at[index, genre] = 1\n",
        "            \n",
        "    #Filling in the NaN values with 0 to show that a movie doesn't have that column's genre\n",
        "    moviesWithGenres_df = moviesWithGenres_df.fillna(0)\n",
        "    moviesWithGenres_df.drop(['genres'],axis =1, inplace = True)\n",
        "    \n",
        "    # Slice the data\n",
        "    movie_subset = moviesWithGenres_df[:subset_size]\n",
        "\n",
        "    return movie_subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QniwlmhtAwd"
      },
      "outputs": [],
      "source": [
        "def content_model(movie_list,top_n=10):\n",
        "    \"\"\"Performs Content filtering based upon a list of movies supplied\n",
        "       by the app user.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    movie_list : list (str)\n",
        "        Favorite movies chosen by the app user.\n",
        "    top_n : type\n",
        "        Number of top recommendations to return to the user.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list (str)\n",
        "        Titles of the top-n movie recommendations to the user.\n",
        "\n",
        "    \"\"\"\n",
        "    # Initializing the empty list of recommended movies\n",
        "    recommended_movies = []\n",
        "    # Preprocess the data \n",
        "    data = data_preprocessing(27000)\n",
        "\n",
        "    # Create a new DataFrame\n",
        "    data2 = data.copy()\n",
        "\n",
        "    # Change the index of the DataFrame\n",
        "    data2 = data2.set_index('title')\n",
        "\n",
        "    # Delete the columns the unneccesary columns \n",
        "    data.drop(['title'],axis = 1,inplace=True)\n",
        "    \n",
        "    # Generating the count matrix for each genres\n",
        "    data = data.set_index('movieId')\n",
        "\n",
        "    # Instantiating the model and fitting the model to the count matrix\n",
        "    nn = NearestNeighbors(algorithm='brute',metric='cosine',n_neighbors=10)\n",
        "    nn.fit(data)\n",
        "\n",
        "    # Create an Empty list for the movieids\n",
        "    movieids = []\n",
        "\n",
        "    # Getting the movieids\n",
        "    ind1 = data2.loc[movie_list[0],'movieId']\n",
        "    ind2 = data2.loc[movie_list[1],'movieId']\n",
        "    ind3 = data2.loc[movie_list[2],'movieId']\n",
        "\n",
        "    # Adding the movieids to the list\n",
        "    movieids.extend([ind1,ind2,ind3])\n",
        "    # Setting a list of indexes\n",
        "    index_list = []\n",
        "\n",
        "    # Getting the suggestions in form of indexes\n",
        "    for i in range(len(movieids)):\n",
        "        distances,suggestions=nn.kneighbors(data.loc[movieids[i],:].values.reshape(1,-1),n_neighbors=15)\n",
        "\n",
        "        for j in range(15):\n",
        "            index_list.append(suggestions[0,j])\n",
        "\n",
        "    # Creating an empty list to store the movie ids \n",
        "    movieids2 = []\n",
        "    \n",
        "    # Create a for loop get the titles of the movies\n",
        "    for i in range(len(index_list)):\n",
        "        id = data.index.values[index_list[i]]\n",
        "        movieids2.append(id)\n",
        "\n",
        "    # Create a list to store all the titles\n",
        "    title_list = []\n",
        "\n",
        "    # Create a new dataframe to get the titles of the movies and change the index to movieId \n",
        "    data3 = data_preprocessing(27000)\n",
        "    data3 = data3.set_index('movieId')\n",
        "\n",
        "    for i in range(len(movieids2)):\n",
        "        title = data3.loc[movieids2[i],'title']\n",
        "        if(title not in movie_list):\n",
        "            title_list.append(title)\n",
        "\n",
        "    # recommended movies \n",
        "    recommended_movies.extend(random.sample(title_list,k = top_n))\n",
        "    return recommended_movies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thM-C-3_tAwd"
      },
      "source": [
        "Demo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkztK6CbtAwd"
      },
      "outputs": [],
      "source": [
        "# Declare a movie list \n",
        "movielist = [\"Toy Story (1995)\",\"Jumanji (1995)\",\"Grumpier Old Men (1995)\"]\n",
        "\n",
        "# Run the recommendation\n",
        "recommendation = content_model(movielist,10)\n",
        "\n",
        "print(recommendation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RO4FCVLtAwd"
      },
      "source": [
        "## 10. Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtsJuOuLtAwd"
      },
      "source": [
        "### Principle Component Analysis\n",
        "High levels of dimensionality of our dataframes creates difficulties such as:\n",
        "\n",
        "*   More storage space required for the data;\n",
        "*   Increased computation time required to work with the data; and\n",
        "*   More features mean more chance of feature correlation, and hence feature redundancy.\n",
        "\n",
        "Principal Component Analysis:<br>\n",
        "The premise of PCA is that data in some higher number of dimensions can be mapped to some lower number of dimensions, whilst retaining the maximum amount of variance in the lower dimension. Lets apply the following steps;\n",
        "\n",
        "* Perform feature scaling on our data;\n",
        "* Construct the covariance matrix of the data;\n",
        "* Compute the eigenvectors of this matrix; and Eigenvectors corresponding to the largest eigenvalues are used to reconstruct a maximal fraction of variance of the original data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxnbYXMPtAwe"
      },
      "outputs": [],
      "source": [
        "# Scaling the movies_df\n",
        "# declare the features to be all columns from our movies_df\n",
        "features = [col for col in train_df.columns]\n",
        "\n",
        "# create scaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# define scaled data frame variable\n",
        "#scaled_df = pd.read_csv('train.csv')  <---- changed after feature engineering - userID 72315 omitted \n",
        "scaled_df = train_df\n",
        "\n",
        "scaled_df[features] = preprocessing.scale(scaled_df[features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7OQUWQntAwe"
      },
      "outputs": [],
      "source": [
        "# define PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# fit the PCA model to our data and apply the dimensionality reduction \n",
        "prin_comp = pca.fit_transform(scaled_df[features])\n",
        "\n",
        "# create a dataframe containing the principal components\n",
        "pca_df = pd.DataFrame(data = prin_comp)\n",
        "\n",
        "\n",
        "\n",
        "# plot line graph of cumulative variance explained\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5SGT6-mtAwe"
      },
      "outputs": [],
      "source": [
        "pca_85 = PCA(.85)\n",
        "pca_85.fit_transform(scaled_df[features])\n",
        "print(round(pca_85.explained_variance_ratio_.sum()*100, 1),\n",
        "      \"% of variance explained by\",\n",
        "      pca_85.n_components_,\n",
        "      \"components.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNZixnM7tAwe"
      },
      "outputs": [],
      "source": [
        "pca.explained_variance_ratio_[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYtOlefytAwe"
      },
      "source": [
        "In this instance, the first component explains 38%, with the 2nd following behind at 25% and the 3rd at 25% as well. Together, these components explain 88% of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgpJ-BDqtAwe"
      },
      "source": [
        "**Putting Our Dimensionality Reduction to Use**\n",
        "\n",
        "Lets build a predictive regressor model.\n",
        "\n",
        "<p align=\"justify\" > We will use the feature named `Rating` as the response variable (the one that we will try to predict). This feature was used in our initial PCA, so we will have to make some changes to the dataset to ensure it is not included this time.\n",
        "\n",
        "<p align=\"justify\" > PCA requires features to be scaled, as we mentioned above. But we don't need to scale the response variable which we will use for prediction, as it is not used in PCA. This is good, because leaving the response as is means any predicted values and associated errors are easier to interpret. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmplQWbstAwf"
      },
      "outputs": [],
      "source": [
        "# exclude non-features from data\n",
        "reg_data = scaled_df[features]\n",
        "\n",
        "# set aside response variable (Unscaled!)\n",
        "reg_response = train_df[\"rating\"]\n",
        "\n",
        "# drop response variable\n",
        "reg_data = reg_data.drop(['rating'], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oEfPiNltAwf"
      },
      "source": [
        "Now, let's split the dataset up into train and test using with a ration of 80 percent training data and 20 percent testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGRc-OdvtAwf"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(reg_data, reg_response, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hj4CgeCtAwf"
      },
      "source": [
        "Next, we apply PCA to the training set with the number of components set to 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TptAlDNAtAwf"
      },
      "outputs": [],
      "source": [
        "# create PCA object with n_components set to 2\n",
        "pca_reg = PCA(n_components=2)\n",
        "\n",
        "# fit the PCA model to our data and apply the dimensionality reduction \n",
        "X_train = pca_reg.fit_transform(X_train)\n",
        "\n",
        "# confirm the number of components\n",
        "pca_reg.n_components_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OUUxyaWtAwf"
      },
      "outputs": [],
      "source": [
        "pca_reg.explained_variance_ratio_.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ezLzlQ-tAwf"
      },
      "source": [
        "## 11. Collaborative Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\" > Collaborative filtering is a technique that can filter out items that a user might like on the basis of reactions by similar users. It works by searching a large group of people and finding a smaller set of users with tastes similar to a particular user. It looks at the items they like and combines them to create a ranked list of suggestions. We shall be applying collaborative based filtering on the testing set and make our submission to the Kaggle competition with it.\n",
        "\n",
        "<p align=\"justify\" > In our modelling we're going to use the surprise package. Surprise is a Python scikit for building and analyzing recommender systems that deal with explicit rating data. The name SurPRISE stands for Simple Python RecommendatIon System Engine.\n",
        "\n",
        "<p align=\"justify\" > Surprise allows us to use a built-in datasets, Movielens. We'll use this data to run experiments on with different models before we decide on a model and use it on our own data.\n",
        "\n",
        "<p align=\"justify\" > The MovieLends dataset is with a smaller dataset than ours, making it easy to implement new algorithm ideas.\n",
        "\n",
        "We'll use the Train-test split and the fit() method to get the RMSE for each algorithm."
      ],
      "metadata": {
        "id": "JozpWSFLw24W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1:**"
      ],
      "metadata": {
        "id": "nrjIliC9xiFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RlCU-IXt43Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 2:**"
      ],
      "metadata": {
        "id": "AzItPYXhxsEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RDoO-4co43uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 3:**"
      ],
      "metadata": {
        "id": "rrUiNiVAxsQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3ULhezQh44e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 4:**"
      ],
      "metadata": {
        "id": "Db5uaIVexwcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c6kjTXUr45WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 5:**"
      ],
      "metadata": {
        "id": "4-vCb6DpxwkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VU58Q_dg46fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Model Comparison based on RMSE**"
      ],
      "metadata": {
        "id": "S-iv8JZByB1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\" > Based on the results of the RMSE, "
      ],
      "metadata": {
        "id": "mKpHuJOy4pZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. The Chosen Model Summary"
      ],
      "metadata": {
        "id": "g4o9EXadyCIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best performing model in this study was the "
      ],
      "metadata": {
        "id": "XbU2N_Ss4aC4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p77wtTPtAwg"
      },
      "source": [
        "## 13. Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaRf9zS5tAwh"
      },
      "outputs": [],
      "source": [
        "# End Comet Experiment \n",
        "experiment.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dKpcd2rtAwh"
      },
      "source": [
        "## 14. Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VymqwqoltAwh"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15. Future Recommendations"
      ],
      "metadata": {
        "id": "g6tLy_b84SWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\" style=\"width: 800px; font-size: 100%; text-align: center; margin: 0 auto\">\n",
        "<img src=\"https://raw.githubusercontent.com/drikus-d/unsupervised-predict-streamlit-template/master/DataSets/f.jpg\"\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "V-JsTkLkF91z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We recommend the following: "
      ],
      "metadata": {
        "id": "UfPqmre94Xho"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Unsupervised Predict Notebook Team 5.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}