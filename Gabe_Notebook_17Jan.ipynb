{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Unsupervised Learning Team 5 Solution (Gabe)\n\n© Explore Data Science Academy\n\n---\n### Problem Statement\n\nIn today’s technology driven world, recommender systems are socially and economically critical for ensuring that individuals can make appropriate choices surrounding the content they engage with on a daily basis. One application where this is especially true surrounds movie content recommendations; where intelligent algorithms can help viewers find great titles from tens of thousands of options.\n\n<img src=\"https://i.pinimg.com/originals/d9/58/5e/d9585efc140b5d3689b3341aa5c35df1.jpg\" alt=\"movie-recommendation\" style=\"width: 800px;\"/>\n\n\n\n","metadata":{"ExecuteTime":{"end_time":"2021-06-11T09:24:53.643384Z","start_time":"2021-06-11T09:24:53.622385Z"}}},{"cell_type":"markdown","source":"\n\nOur team has been challenged with constructing a movie recommendation algorithm based on content or collaborative filtering, capable of accurately predicting how a user will rate a movie they have not yet viewed based on their historical preferences.\n\nProviding an accurate and robust solution to this challenge has immense economic potential, with users of the system being exposed to content they would like to view or purchase - generating revenue and platform affinity.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"cont\"></a>\n\n## Table of Contents\n\n<a href=#one>(i) Comet Experiment</a>\n\n<a href=#one>1. Importing Packages</a>\n\n<a href=#two>2. Loading Data and Data Descriptions</a>\n\n<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n\n<a href=#four>4. Data Engineering</a>\n\n<a href=#five>5. Modeling</a>\n\n<a href=#six>6. Model Performance</a>\n\n<a href=#seven>7. Model Explanations</a>","metadata":{}},{"cell_type":"markdown","source":" <a id=\"one\"></a>\n## (i) Comet Experiment\n<a href=#cont>Back to Table of Contents</a>\n\n---\n    \n| ⚡ Description: Importing Packages ⚡ |\n| :--------------------------- |\n| In this section we conduct our Comet Experiment. |\n\n---","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <a id=\"one\"></a>\n## 1. Importing Packages\n<a href=#cont>Back to Table of Contents</a>\n\n---\n    \n| ⚡ Description: Importing Packages ⚡ |\n| :--------------------------- |\n| In this section we import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n\n---","metadata":{}},{"cell_type":"code","source":"# data analysis libraries\nimport pandas as pd\nimport numpy as np\n\n# Kaggle requirements\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# visualisation libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom numpy.random import RandomState\nfrom plotly.offline import init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\n\n#word cloud\n%matplotlib inline\nimport wordcloud\n\nfrom wordcloud import WordCloud, STOPWORDS\n%matplotlib inline\nsns.set()\n\n\n'''\n# ML Models\n!pip install scikit surprise\nfrom surprise import Reader\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\nfrom surprise import NormalPredictor\nfrom surprise import KNNBasic\nfrom surprise import KNNWithMeans\nfrom surprise import KNNWithZScore\nfrom surprise import KNNBaseline\nfrom surprise import SVD\nfrom surprise import BaselineOnly\nfrom surprise import SVDpp\nfrom surprise import NMF\nfrom surprise import SlopeOne\nfrom surprise import CoClustering\nfrom surprise.accuracy import rmse\nfrom surprise import accuracy\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# ML Pre processing\nfrom surprise.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Hyperparameter tuning\nfrom surprise.model_selection import GridSearchCV\n'''\n\n# High performance hyperparameter tuning\n#from tune_sklearn import TuneSearchCV\n\n# Remove warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"ExecuteTime":{"end_time":"2021-06-23T10:30:53.800892Z","start_time":"2021-06-23T10:30:50.215449Z"},"execution":{"iopub.status.busy":"2022-01-17T09:05:32.890742Z","iopub.execute_input":"2022-01-17T09:05:32.891069Z","iopub.status.idle":"2022-01-17T09:05:34.021050Z","shell.execute_reply.started":"2022-01-17T09:05:32.890983Z","shell.execute_reply":"2022-01-17T09:05:34.020298Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"two\"></a>\n## 2. Loading the Data and Data Descriptions\n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>\n\n---\n    \n| ⚡ Description: Loading the data and data descriptions⚡ |\n| :--------------------------- |\n| In this section we load the data from the CSV files into a DataFrame. We also descripe the various csv files|\n\n---","metadata":{}},{"cell_type":"code","source":"# read in all csv files \ntrain = pd.read_csv('../input/edsa-movie-recommendation-wilderness/train.csv')\ntest = pd.read_csv('../input/edsa-movie-recommendation-wilderness/test.csv')\ngenome_scores = pd.read_csv('../input/edsa-movie-recommendation-wilderness/genome_scores.csv')\ngenome_tags = pd.read_csv('../input/edsa-movie-recommendation-wilderness/genome_tags.csv')\nimdb_data = pd.read_csv('../input/edsa-movie-recommendation-wilderness/imdb_data.csv')\nlinks = pd.read_csv('../input/edsa-movie-recommendation-wilderness/links.csv')\nmovies = pd.read_csv('../input/edsa-movie-recommendation-wilderness/movies.csv')\ntags = pd.read_csv('../input/edsa-movie-recommendation-wilderness/tags.csv')","metadata":{"ExecuteTime":{"end_time":"2021-06-28T08:49:35.311495Z","start_time":"2021-06-28T08:49:35.295494Z"},"execution":{"iopub.status.busy":"2022-01-17T09:10:06.532481Z","iopub.execute_input":"2022-01-17T09:10:06.533242Z","iopub.status.idle":"2022-01-17T09:10:22.745998Z","shell.execute_reply.started":"2022-01-17T09:10:06.533191Z","shell.execute_reply":"2022-01-17T09:10:22.745353Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### Dataset Descriptions\nThe supplied dataset comprises the following:\n\n1. genome_scores.csv - A score mapping the strength between movies and tag-related properties\n2. train.csv - The training split of the dataset. Contains user and movie IDs with associated rating data\n3. test.csv - The test split of the dataset. Contains user and movie IDs with no rating data\n3. tags.csv - User assigned for the movies within the dataset\n3. links.csv - File providing a mapping between a movie ID, IMDB IDs and TMDB IDs\n4. movies - File providing details about the title of the movie, genres and movieID that further can be used 5. to merge to other related dataset\n6. imdb_data.csv - Additional movie metadata scraped from IMDB using the links.csv file\n7. genome_tags.csv - User assigned tags for genome-related scores","metadata":{}},{"cell_type":"markdown","source":"<a id=\"three\"></a>\n## 3. Exploratory Data Analysis (EDA)\n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>\n\n---\n    \n| ⚡ Description: Exploratory data analysis ⚡ |\n| :--------------------------- |\n| In this section we perform an in-depth analysis of all the variables in the various dataFrames. |\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"Let's first take a look at the shape of all the datasets in order to have a general overview.","metadata":{}},{"cell_type":"code","source":"# Declaring a list that contains the names of the dataframes\ndfs = [train, test, genome_scores, genome_tags, imdb_data, links, movies, tags]\n# Create a list of the names of the imported datasets\ndf_names = ['train', 'test', 'genome_scores', 'genome_tags',\n            'imdb_data', 'links', 'movies', 'tags']\ndfs_dict = {}  # declaring an empty dictionary\nfor name, data in zip(df_names, dfs):  # iterate over the list and dictionary\n    dfs_dict[name] = [data.shape[0], data.shape[1]]\n    df_prop = pd.DataFrame(dfs_dict,\n                          index=['rows', 'columns']).transpose()\ndf_properties = df_prop.sort_values(by='rows', ascending=False)\n\ndf_properties  # view the final output","metadata":{"ExecuteTime":{"end_time":"2021-06-28T08:52:37.824204Z","start_time":"2021-06-28T08:52:37.811206Z"},"execution":{"iopub.status.busy":"2022-01-17T09:10:49.539019Z","iopub.execute_input":"2022-01-17T09:10:49.539689Z","iopub.status.idle":"2022-01-17T09:10:49.585880Z","shell.execute_reply.started":"2022-01-17T09:10:49.539639Z","shell.execute_reply":"2022-01-17T09:10:49.585229Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### Viewing of data and missing values ","metadata":{}},{"cell_type":"markdown","source":"Let's take a quick 'sneak peek' and some basic information at each of the datasets provided. ","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:55:29.143856Z","iopub.execute_input":"2022-01-17T09:55:29.144119Z","iopub.status.idle":"2022-01-17T09:55:29.155554Z","shell.execute_reply.started":"2022-01-17T09:55:29.144091Z","shell.execute_reply":"2022-01-17T09:55:29.154614Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print('INFO OF DATASET')\ntrain.info()\nprint('    ----------------')\nprint('MISSING VALUES OF DATASET')\ntrain.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:10:58.002338Z","iopub.execute_input":"2022-01-17T09:10:58.002611Z","iopub.status.idle":"2022-01-17T09:10:58.090097Z","shell.execute_reply.started":"2022-01-17T09:10:58.002582Z","shell.execute_reply":"2022-01-17T09:10:58.089188Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:55:25.529543Z","iopub.execute_input":"2022-01-17T09:55:25.529980Z","iopub.status.idle":"2022-01-17T09:55:25.538684Z","shell.execute_reply.started":"2022-01-17T09:55:25.529933Z","shell.execute_reply":"2022-01-17T09:55:25.537832Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"print('INFO OF DATASET')\ntest.info()\nprint('    ----------------')\nprint('MISSING VALUES OF DATASET')\ntest.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:11:04.921364Z","iopub.execute_input":"2022-01-17T09:11:04.921633Z","iopub.status.idle":"2022-01-17T09:11:04.948926Z","shell.execute_reply.started":"2022-01-17T09:11:04.921604Z","shell.execute_reply":"2022-01-17T09:11:04.948085Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"genome_scores.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:55:20.088695Z","iopub.execute_input":"2022-01-17T09:55:20.089016Z","iopub.status.idle":"2022-01-17T09:55:20.101142Z","shell.execute_reply.started":"2022-01-17T09:55:20.088985Z","shell.execute_reply":"2022-01-17T09:55:20.100237Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print('INFO OF DATASET')\ngenome_scores.info()\nprint('    ----------------')\nprint('MISSING VALUES OF DATASET')\ngenome_scores.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:11:09.857057Z","iopub.execute_input":"2022-01-17T09:11:09.857352Z","iopub.status.idle":"2022-01-17T09:11:09.944148Z","shell.execute_reply.started":"2022-01-17T09:11:09.857314Z","shell.execute_reply":"2022-01-17T09:11:09.943244Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"genome_tags.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:55:35.909618Z","iopub.execute_input":"2022-01-17T09:55:35.910518Z","iopub.status.idle":"2022-01-17T09:55:35.919378Z","shell.execute_reply.started":"2022-01-17T09:55:35.910479Z","shell.execute_reply":"2022-01-17T09:55:35.918565Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"print('INFO OF DATASET')\ngenome_tags.info()\nprint('    ----------------')\nprint('MISSING VALUES OF DATASET')\ngenome_tags.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:11:15.089012Z","iopub.execute_input":"2022-01-17T09:11:15.089827Z","iopub.status.idle":"2022-01-17T09:11:15.107523Z","shell.execute_reply.started":"2022-01-17T09:11:15.089774Z","shell.execute_reply":"2022-01-17T09:11:15.106648Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"imdb_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:55:38.716049Z","iopub.execute_input":"2022-01-17T09:55:38.716458Z","iopub.status.idle":"2022-01-17T09:55:38.728992Z","shell.execute_reply.started":"2022-01-17T09:55:38.716428Z","shell.execute_reply":"2022-01-17T09:55:38.728041Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"print('INFO OF DATASET')\nimdb_data.info()\nprint('    ----------------')\nprint('MISSING VALUES OF DATASET')\nimdb_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:11:19.733309Z","iopub.execute_input":"2022-01-17T09:11:19.733802Z","iopub.status.idle":"2022-01-17T09:11:19.773197Z","shell.execute_reply.started":"2022-01-17T09:11:19.733753Z","shell.execute_reply":"2022-01-17T09:11:19.772546Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We clearly see a number of missing data for each of the columns. Let's allow for a more visual representation of the missing data below...","metadata":{}},{"cell_type":"code","source":"import missingno as msno\n\n# plot bar chart of the missing values\nmsno.bar(imdb_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:17:22.423821Z","iopub.execute_input":"2022-01-17T09:17:22.424080Z","iopub.status.idle":"2022-01-17T09:17:23.257987Z","shell.execute_reply.started":"2022-01-17T09:17:22.424051Z","shell.execute_reply":"2022-01-17T09:17:23.257202Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"The bar graph above gives a clear visual representation of the extent of missing data for each column. It should be noted that for the 'Budget' column, more than half the data is missing. If need be, we'll look to address all these issues at a later stage. For now, let's take a look at the distribution of the missing data below.. ","metadata":{}},{"cell_type":"code","source":"# plot a matrix of the missing data \nmsno.matrix(imdb_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:26:35.024140Z","iopub.execute_input":"2022-01-17T09:26:35.024929Z","iopub.status.idle":"2022-01-17T09:26:35.630587Z","shell.execute_reply.started":"2022-01-17T09:26:35.024871Z","shell.execute_reply":"2022-01-17T09:26:35.629493Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"We see that the missing data is quite evenly distributed across the various columns. ","metadata":{}},{"cell_type":"code","source":"links.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:11:22.250748Z","iopub.execute_input":"2022-01-17T09:11:22.251486Z","iopub.status.idle":"2022-01-17T09:11:22.263580Z","shell.execute_reply.started":"2022-01-17T09:11:22.251440Z","shell.execute_reply":"2022-01-17T09:11:22.262905Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print('INFO OF DATASET')\nlinks.info()\nprint('    ----------------')\nprint('MISSING VALUES OF DATASET')\nlinks.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:11:23.940019Z","iopub.execute_input":"2022-01-17T09:11:23.940676Z","iopub.status.idle":"2022-01-17T09:11:23.958317Z","shell.execute_reply.started":"2022-01-17T09:11:23.940628Z","shell.execute_reply":"2022-01-17T09:11:23.957453Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"As before, let's get a more visual look of the missing data.. ","metadata":{}},{"cell_type":"code","source":"# plot bar chart of the missing values\nmsno.bar(links)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:29:38.502765Z","iopub.execute_input":"2022-01-17T09:29:38.503547Z","iopub.status.idle":"2022-01-17T09:29:39.164014Z","shell.execute_reply.started":"2022-01-17T09:29:38.503507Z","shell.execute_reply":"2022-01-17T09:29:39.163164Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Only a slight fraction of data is missing in the 'tmdbId' column.","metadata":{}},{"cell_type":"code","source":"# plot a matrix of the missing data \nmsno.matrix(links)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:35:40.628287Z","iopub.execute_input":"2022-01-17T09:35:40.628564Z","iopub.status.idle":"2022-01-17T09:35:41.205710Z","shell.execute_reply.started":"2022-01-17T09:35:40.628536Z","shell.execute_reply":"2022-01-17T09:35:41.204782Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"The matrix above shows us that there's only single small section in the whole 'tmdbId' column that has missing data. ","metadata":{}},{"cell_type":"code","source":"movies.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:11:27.130471Z","iopub.execute_input":"2022-01-17T09:11:27.131299Z","iopub.status.idle":"2022-01-17T09:11:27.142929Z","shell.execute_reply.started":"2022-01-17T09:11:27.131239Z","shell.execute_reply":"2022-01-17T09:11:27.142069Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print('INFO OF DATASET')\nmovies.info()\nprint('    ----------------')\nprint('MISSING VALUES OF DATASET')\nmovies.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:11:28.865891Z","iopub.execute_input":"2022-01-17T09:11:28.866639Z","iopub.status.idle":"2022-01-17T09:11:28.915070Z","shell.execute_reply.started":"2022-01-17T09:11:28.866595Z","shell.execute_reply":"2022-01-17T09:11:28.914130Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"tags.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:11:30.984845Z","iopub.execute_input":"2022-01-17T09:11:30.985372Z","iopub.status.idle":"2022-01-17T09:11:30.995826Z","shell.execute_reply.started":"2022-01-17T09:11:30.985338Z","shell.execute_reply":"2022-01-17T09:11:30.994995Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print('INFO OF DATASET')\ntags.info()\nprint('    ----------------')\nprint('MISSING VALUES OF DATASET')\ntags.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:11:32.909062Z","iopub.execute_input":"2022-01-17T09:11:32.909348Z","iopub.status.idle":"2022-01-17T09:11:33.179742Z","shell.execute_reply.started":"2022-01-17T09:11:32.909317Z","shell.execute_reply":"2022-01-17T09:11:33.178961Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Only 16 rows missing for the 'tag' column. For consistency's sake, let's give a visual representation of this below...","metadata":{}},{"cell_type":"code","source":"# plot bar chart of the missing values\nmsno.bar(tags)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:44:38.107247Z","iopub.execute_input":"2022-01-17T09:44:38.107536Z","iopub.status.idle":"2022-01-17T09:44:39.318309Z","shell.execute_reply.started":"2022-01-17T09:44:38.107506Z","shell.execute_reply":"2022-01-17T09:44:39.317703Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"As expected, the missing data represented visually is practically negligible (bar graph above). ","metadata":{}},{"cell_type":"code","source":"# plot a matrix of the missing data \nmsno.matrix(tags)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:46:11.985175Z","iopub.execute_input":"2022-01-17T09:46:11.985809Z","iopub.status.idle":"2022-01-17T09:46:15.202813Z","shell.execute_reply.started":"2022-01-17T09:46:11.985772Z","shell.execute_reply":"2022-01-17T09:46:15.202124Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"The missing data is not even noticeable if we represent is via a matrix. Let's provide a summary of our findings of all dataframes below.","metadata":{}},{"cell_type":"markdown","source":"Upon investigation of missing values, we have found the following: \n\n* The links dataset has 107 missing values in the tmdb column. This makes up for a total of only 0.17% of missing data.\n* The tags dataset has 16 missing values in the tag column. This makes up for a total of only 0.00015% of the missing data\n*  The imdb_data dataset has a range of missing values - if need be, we'll address this issue at a later stage.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# have a look at feature distributions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"four\"></a>\n## 4. Data Engineering\n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>\n\n---\n    \n| ⚡ Description: Data engineering ⚡ |\n| :--------------------------- |\n| In this section we clean the dataset, and possibly create new features - as identified in the EDA phase. |\n\n---","metadata":{}},{"cell_type":"code","source":"# remove missing values/ features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create new features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# engineer existing features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"five\"></a>\n## 5. Modelling\n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>\n\n---\n    \n| ⚡ Description: Modelling ⚡ |\n| :--------------------------- |\n| In this section we create one or more models. |\n\n---","metadata":{}},{"cell_type":"code","source":"# split data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create targets and features dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create one or more ML models","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate one or more ML models","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"six\"></a>\n## 6. Model Performance\n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>\n\n---\n    \n| ⚡ Description: Model performance ⚡ |\n| :--------------------------- |\n| In this section we compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n\n---","metadata":{}},{"cell_type":"code","source":"# Compare model performance","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose best model and motivate why it is the best choice","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"seven\"></a>\n## 7. Model Explanations\n<a class=\"anchor\" id=\"1.1\"></a>\n<a href=#cont>Back to Table of Contents</a>\n\n---\n    \n| ⚡ Description: Model explanation ⚡ |\n| :--------------------------- |\n| In this section we discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n\n---","metadata":{}},{"cell_type":"code","source":"# discuss chosen methods logic","metadata":{},"execution_count":null,"outputs":[]}]}